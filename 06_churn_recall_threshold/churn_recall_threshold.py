"""
üéØ Threshold - Balansowanie Recall vs Precision w Churn Prediction

Cel: Pokazaƒá jak threshold wp≈Çywa na trade-off miƒôdzy wykrywaniem
     klient√≥w a liczbƒÖ fa≈Çszywych alarm√≥w.
"""

# Import bibliotek
import pandas as pd
from pycaret.classification import *
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import confusion_matrix

print("‚úÖ Biblioteki zaimportowane!")

# Wczytanie i przygotowanie danych
df = pd.read_csv('data/WA_Fn-UseC_-Telco-Customer-Churn.csv')

print(f"\nüìä Liczba klient√≥w: {len(df)}")
print(f"üìã Liczba kolumn: {len(df.columns)}")

# Przygotowanie danych
df = df.drop('customerID', axis=1)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'].fillna(0, inplace=True)

print("\nüìä Rozk≈Çad churn:")
print(df['Churn'].value_counts())
print(f"\nProcent odchodzƒÖcych: {(df['Churn'] == 'Yes').sum() / len(df) * 100:.1f}%")

# Konfiguracja PyCaret
clf_setup = setup(
    data=df,
    target='Churn',
    session_id=123,
    train_size=0.8,
    fold=5,
    normalize=True,
    verbose=False
)

print("\n‚úÖ PyCaret skonfigurowany!")

# Trening modelu z optymalizacjƒÖ pod Recall
print("\nüîÑ Por√≥wnywanie modeli z optymalizacjƒÖ pod RECALL...\n")

best_model = compare_models(sort='Recall', n_select=1)

print("\n‚úÖ Najlepszy model (pod kƒÖtem Recall) wybrany!")

# Tworzenie finalizowanego modelu
print("\nüìä Tworzenie modelu...")
final_model = create_model(best_model)

# Wy≈õwietlanie confusion matrix
print("\nüìä Wy≈õwietlanie Confusion Matrix...")
plot_model(final_model, plot='confusion_matrix')

# ============================================================================
# TUNING MODELU POD KƒÑTEM RECALL
# ============================================================================
print("\n" + "="*80)
print("üîß TUNING - Optymalizacja modelu pod kƒÖtem RECALL")
print("="*80)

# Zapisujemy metryki modelu bazowego
baseline_metrics = pull()
baseline_recall = baseline_metrics.loc['Mean', 'Recall']
baseline_precision = baseline_metrics.loc['Mean', 'Prec.']
baseline_accuracy = baseline_metrics.loc['Mean', 'Accuracy']
baseline_auc = baseline_metrics.loc['Mean', 'AUC']

print(f"\nüìà Model bazowy (przed tuningiem):")
print(f"   Recall:    {baseline_recall:.4f}")
print(f"   Precision: {baseline_precision:.4f}")
print(f"   Accuracy:  {baseline_accuracy:.4f}")
print(f"   AUC:       {baseline_auc:.4f}")

# Tunujemy model pod kƒÖtem Recall
print("\nSzukamy najlepszych hiperparametr√≥w...")
tuned_model = tune_model(final_model, optimize='Recall', n_iter=20)

# Pobieramy metryki po tuningu
tuned_metrics = pull()
tuned_recall = tuned_metrics.loc['Mean', 'Recall']
tuned_precision = tuned_metrics.loc['Mean', 'Prec.']
tuned_accuracy = tuned_metrics.loc['Mean', 'Accuracy']
tuned_auc = tuned_metrics.loc['Mean', 'AUC']

print(f"\nüìà Model po tuningu:")
print(f"   Recall:    {tuned_recall:.4f}")
print(f"   Precision: {tuned_precision:.4f}")
print(f"   Accuracy:  {tuned_accuracy:.4f}")
print(f"   AUC:       {tuned_auc:.4f}")

# Por√≥wnanie i decyzja
recall_diff = (tuned_recall - baseline_recall) * 100

print(f"\n‚öñÔ∏è Zmiana Recall: {recall_diff:+.2f} p.p.")

THRESHOLD_FOR_IMPROVEMENT = 1.0  # 1 punkt procentowy

if recall_diff >= THRESHOLD_FOR_IMPROVEMENT:
    print(f"‚úÖ TUNING OP≈ÅACALNY! U≈ºywamy tuned_model")
    selected_model = tuned_model
    model_name = "TUNED"
else:
    print(f"‚ùå TUNING NIE WART ZACHODU! U≈ºywamy modelu bazowego")
    selected_model = final_model
    model_name = "BAZOWY"

print(f"\nüí° Wybrany model: {model_name}")
print("="*80)

# Pobieranie danych testowych
X_test = get_config('X_test')
y_test = get_config('y_test')
y_test_numeric = (y_test == 'Yes').astype(int)

# Przewidywania (u≈ºywamy wybranego modelu)
print(f"\nü§ñ U≈ºywamy modelu: {model_name}")
predictions = predict_model(selected_model, data=X_test)
y_proba = predictions['prediction_score'].values

print(f"\n‚úÖ Pobrano {len(X_test)} przyk≈Çad√≥w testowych")

# Funkcja do obliczania confusion matrix
def calculate_confusion_matrix(y_true, y_proba, threshold):
    """Oblicza confusion matrix dla danego threshold."""
    y_pred = (y_proba >= threshold).astype(int)
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    
    return {
        'threshold': threshold,
        'confusion_matrix': cm,
        'tn': tn,
        'fp': fp,
        'fn': fn,
        'tp': tp,
        'recall': recall,
        'precision': precision,
        'accuracy': accuracy
    }

# Testowanie r√≥≈ºnych threshold√≥w
thresholds = [0.3, 0.5, 0.7]
results = []

for threshold in thresholds:
    result = calculate_confusion_matrix(y_test_numeric, y_proba, threshold)
    results.append(result)

print("\n‚úÖ Obliczono confusion matrix dla wszystkich threshold√≥w!")

# Wizualizacja
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, result in enumerate(results):
    cm = result['confusion_matrix']
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                xticklabels=['Zostaje', 'Odchodzi'],
                yticklabels=['Zostaje', 'Odchodzi'])
    
    axes[idx].set_title(
        f"Threshold = {result['threshold']}\n"
        f"Recall: {result['recall']:.2%} | Precision: {result['precision']:.2%}",
        fontsize=12, fontweight='bold'
    )
    
    axes[idx].set_ylabel('Rzeczywisto≈õƒá')
    axes[idx].set_xlabel('Przewidywanie')

plt.tight_layout()
plt.show()

print("\nüìä Confusion Matrix dla r√≥≈ºnych threshold√≥w wy≈õwietlona!")

# Szczeg√≥≈Çowe por√≥wnanie
print("\n" + "="*80)
print("üìä SZCZEG√ì≈ÅOWE POR√ìWNANIE THRESHOLD√ìW")
print("="*80)

for result in results:
    print(f"\n{'='*80}")
    print(f"üéØ THRESHOLD = {result['threshold']} ({result['threshold']*100:.0f}%)")
    print(f"{'='*80}")
    
    print(f"\nüìà CONFUSION MATRIX:")
    print(f"   ‚úÖ True Negative (TN):  {result['tn']:4d} - Prawid≈Çowo: 'klient zostaje'")
    print(f"   ‚ö†Ô∏è  False Positive (FP): {result['fp']:4d} - Fa≈Çszywy alarm")
    print(f"   ‚ùå False Negative (FN): {result['fn']:4d} - Przegapiony")
    print(f"   ‚úÖ True Positive (TP):  {result['tp']:4d} - Prawid≈Çowo wykryty")
    
    print(f"\nüìä METRYKI:")
    print(f"   Recall:    {result['recall']:.2%}")
    print(f"   Precision: {result['precision']:.2%}")
    print(f"   Accuracy:  {result['accuracy']:.2%}")

print(f"\n{'='*80}")

# Analiza biznesowa
cost_false_positive = 20  # z≈Ç
cost_false_negative = 500  # z≈Ç
retention_rate = 0.30
value_retained_customer = 500  # z≈Ç
cost_retention_attempt = 50  # z≈Ç

print("\n" + "="*80)
print("üí∞ ANALIZA BIZNESOWA")
print("="*80)

print(f"\nüìã Za≈Ço≈ºenia:")
print(f"   - Koszt fa≈Çszywego alarmu (FP): {cost_false_positive} z≈Ç")
print(f"   - Koszt przegapienia (FN): {cost_false_negative} z≈Ç")
print(f"   - Koszt retencji: {cost_retention_attempt} z≈Ç")
print(f"   - Skuteczno≈õƒá retencji: {retention_rate*100:.0f}%")

for result in results:
    print(f"\n{'='*80}")
    print(f"üéØ THRESHOLD = {result['threshold']}")
    print(f"{'='*80}")
    
    cost_fp = result['fp'] * cost_false_positive
    cost_fn = result['fn'] * cost_false_negative
    cost_tp = result['tp'] * cost_retention_attempt
    revenue_tp = result['tp'] * retention_rate * value_retained_customer
    
    total_cost = cost_fp + cost_fn + cost_tp
    total_revenue = revenue_tp
    net_profit = total_revenue - total_cost
    
    print(f"\nüí∏ KOSZTY:")
    print(f"   Fa≈Çszywe alarmy (FP={result['fp']}): {cost_fp:,} z≈Ç")
    print(f"   Przegapieni (FN={result['fn']}): {cost_fn:,} z≈Ç")
    print(f"   Pr√≥by retencji (TP={result['tp']}): {cost_tp:,} z≈Ç")
    print(f"   SUMA: {total_cost:,} z≈Ç")
    
    print(f"\nüí∞ PRZYCHODY:")
    print(f"   Zatrzymani ({result['tp']*retention_rate:.0f} z {result['tp']}): {revenue_tp:,} z≈Ç")
    
    print(f"\nüìä BILANS:")
    if net_profit >= 0:
        print(f"   ‚úÖ ZYSK: {net_profit:,} z≈Ç")
    else:
        print(f"   ‚ùå STRATA: {abs(net_profit):,} z≈Ç")

print(f"\n{'='*80}")

# Wnioski
print("\n" + "="*80)
print("üéØ WNIOSKI")
print("="*80)

print("""
üìä TRADE-OFF RECALL vs PRECISION:

1Ô∏è‚É£ THRESHOLD 0.3 (Liberalny):
   ‚úÖ Wysoki Recall - wykrywamy wiƒôcej odchodzƒÖcych
   ‚ùå Niski Precision - wiƒôcej fa≈Çszywych alarm√≥w
   üí° U≈ºywaj gdy: Koszt przegapienia >> Koszt fa≈Çszywego alarmu

2Ô∏è‚É£ THRESHOLD 0.5 (Standardowy):
   ‚öñÔ∏è Balans miƒôdzy Recall a Precision
   üí° RozsƒÖdny kompromis

3Ô∏è‚É£ THRESHOLD 0.7 (Konserwatywny):
   ‚úÖ Wysoki Precision - ma≈Ço fa≈Çszywych alarm√≥w
   ‚ùå Niski Recall - przegapiamy wiƒôcej klient√≥w
   üí° U≈ºywaj gdy: Koszt fa≈Çszywego alarmu >> Koszt przegapienia

üí° W churn prediction zazwyczaj LEPIEJ:
   - Ni≈ºszy threshold (0.3-0.4)
   - Wysoki Recall (wykryƒá wiƒôcej)
   - Ni≈ºszy Precision (tolerowaƒá fa≈Çszywe alarmy)
   
   DLACZEGO? Koszt przegapienia (500 z≈Ç) >> Koszt alarmu (20 z≈Ç)
   
   ANALOGIA: Wykrywacz dymu - wolisz 10 fa≈Çszywych alarm√≥w
            ni≈º przegapiƒá prawdziwy po≈ºar!
""")

# Analiza rozk≈Çadu prawdopodobie≈Ñstw
print("\n" + "="*80)
print("‚ö†Ô∏è ROZK≈ÅAD PRAWDOPODOBIE≈ÉSTW")
print("="*80)

count_below_03 = (y_proba < 0.3).sum()
count_03_05 = ((y_proba >= 0.3) & (y_proba < 0.5)).sum()
count_05_07 = ((y_proba >= 0.5) & (y_proba < 0.7)).sum()
count_above_07 = (y_proba >= 0.7).sum()

print(f"\n  < 0.3:  {count_below_03} klient√≥w")
print(f"  0.3-0.5: {count_03_05} klient√≥w")
print(f"  0.5-0.7: {count_05_07} klient√≥w")
print(f"  ‚â• 0.7:  {count_above_07} klient√≥w")

if count_03_05 == 0 and count_below_03 == 0:
    print("\nüí° Model ma POLARYZOWANY rozk≈Çad (wszystkie ‚â• 0.5)")
    print("   ‚Üí Threshold 0.3 i 0.5 dajƒÖ IDENTYCZNE wyniki!")
    print("   ‚Üí Zostaw threshold 0.5 (domy≈õlny)")

print("="*80)
print("üéâ ANALIZA ZAKO≈ÉCZONA!")
print("="*80)
