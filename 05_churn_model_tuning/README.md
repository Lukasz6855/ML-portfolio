# ğŸ¯ Tuning Modelu - Czy Warto?

## ğŸ“‹ Spis treÅ›ci
- [Cel projektu](#-cel-projektu)
- [Technologie](#-technologie)
- [Kluczowe pytanie](#-kluczowe-pytanie-tuning--czy-warto)
- [Wyniki eksperymentu](#-wyniki-eksperymentu)
- [Analiza biznesowa](#-analiza-biznesowa)
- [Wnioski](#-wnioski)
- [Jak uruchomiÄ‡](#-jak-uruchomiÄ‡)

## ğŸ¯ Cel projektu

Projekt odpowiada na **fundamentalne pytanie** w Machine Learning:

> **Czy tuning modelu (optymalizacja hiperparametrÃ³w) daje realnÄ… wartoÅ›Ä‡ biznesowÄ…?**

Wiele osÃ³b wykonuje tuning "bo tak trzeba", nie zastanawiajÄ…c siÄ™ czy:
- âœ… Poprawa techniczna jest istotna
- âœ… PrzekÅ‚ada siÄ™ na korzyÅ›ci biznesowe
- âœ… Czas poÅ›wiÄ™cony na tuning jest uzasadniony

Ten projekt przeprowadza **rzetelnÄ… analizÄ™ techniczno-biznesowÄ…** na rzeczywistych danych.

## ğŸ›  Technologie

- **Python 3.8+**
- **PyCaret 3.x** - AutoML do klasyfikacji
- **Pandas** - manipulacja danymi
- **NumPy** - obliczenia numeryczne

### Dataset
**Telco Customer Churn** - dane o klientach firmy telekomunikacyjnej:
- 7043 klientÃ³w
- 20 cech (tenure, contract, monthly charges, itp.)
- Target: Churn (Yes/No) - 27% klientÃ³w odchodzi

## â“ Kluczowe pytanie: Tuning â€“ Czy warto?

### Metodologia

1. **Model bazowy** - Gradient Boosting z domyÅ›lnymi ustawieniami
2. **Model po tuningu** - 20 iteracji optymalizacji z Optuna
3. **PorÃ³wnanie metryk** - Accuracy, AUC, Recall, Precision
4. **Analiza biznesowa** - Obliczenie ROI i zysku/straty

### Co testujemy?

```python
# Model PRZED tuningiem (domyÅ›lne ustawienia)
model_before = create_model('gbc', fold=5)

# Model PO tuningu (optymalizacja 20 konfiguracji)
model_after = tune_model(model_before, optimize='AUC', n_iter=20)
```

## ğŸ“Š Wyniki eksperymentu

### Metryki techniczne

| Metryka | Przed tuningiem | Po tuningu | Zmiana |
|---------|----------------|------------|--------|
| **Accuracy** | 79.93% | 79.41% | **-0.52 p.p.** âš ï¸ |
| **AUC** | 0.8463 | 0.8493 | **+0.30 p.p.** âœ… |
| **Recall** | 79.93% | 79.41% | **-0.52 p.p.** âš ï¸ |
| **Precision** | 79.03% | 78.21% | **-0.82 p.p.** âš ï¸ |

### ğŸ” Interpretacja metryk

**AUC (0.8463 â†’ 0.8493):**
- âœ… Nieznaczna poprawa o 0.3 punktu procentowego
- Model nieco lepiej rozrÃ³Å¼nia klientÃ³w odchodzÄ…cych vs zostajÄ…cych
- **Ale:** Poprawa jest minimalna

**Accuracy, Recall, Precision:**
- âŒ **Wszystkie spadÅ‚y** po tuningu!
- Model po tuningu wykrywa **MNIEJ** klientÃ³w zagroÅ¼onych odejÅ›ciem
- Precision spadÅ‚a - wiÄ™cej faÅ‚szywych alarmÃ³w

## ğŸ’° Analiza biznesowa

### ZaÅ‚oÅ¼enia

```python
Baza klientÃ³w: 10,000
Klienci odchodzÄ…cy rocznie: 2,700 (27%)
Koszt prÃ³by zatrzymania klienta: 50 zÅ‚ (telefon + oferta)
WartoÅ›Ä‡ klienta rocznie: 500 zÅ‚
SkutecznoÅ›Ä‡ retencji: 30% (zatrzymujemy 30% wykrytych)
```

### Wykrywanie klientÃ³w

| | Przed tuningiem | Po tuningu | RÃ³Å¼nica |
|---|----------------|-----------|---------|
| **Wykryci klienci** | 2,158 | 2,144 | **-14** âŒ |
| **Zatrzymani (30%)** | 647 | 643 | **-4** âŒ |

### Bilans finansowy

```
ğŸ“‰ STRATA Z TUNINGU:

Dodatkowy koszt:     -700 zÅ‚  (14 klientÃ³w mniej Ã— 50 zÅ‚)
Dodatkowy przychÃ³d:  -2,000 zÅ‚ (4 klientÃ³w mniej Ã— 500 zÅ‚)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STRATA NETTO:        -1,300 zÅ‚ rocznie
```

### ğŸ“ˆ ROI (Return on Investment)

```
ROI = -185% (strata!)

Inwestujemy czas w tuning â†’ tracÄ™ 1,300 zÅ‚ rocznie
```

## ğŸ¯ Wnioski

### 1. Czy tuning poprawiÅ‚ model technicznie?

**âš ï¸ Minimalna poprawa, praktycznie bez znaczenia**

- AUC wzrosÅ‚o o 0.30 p.p. - nieistotne statystycznie
- Accuracy, Recall, Precision **spadÅ‚y**
- Model po tuningu jest **gorszy** w wykrywaniu klientÃ³w

### 2. Czy tuning ma sens biznesowy?

**âŒ NIE - Model po tuningu GORZEJ wykrywa klientÃ³w i generuje STRATÄ˜**

- Wykrywamy **14 klientÃ³w MNIEJ** rocznie
- Zatrzymujemy **4 klientÃ³w MNIEJ**
- **Strata: 1,300 zÅ‚ rocznie**

### 3. Kiedy tuning MA SENS?

Tuning jest wart czasu i wysiÅ‚ku gdy:

âœ… **Poprawa AUC > 1 punkt procentowy** (u nas: 0.30 p.p.)
âœ… **Recall/Precision rosnÄ…** (u nas: spadÅ‚y!)
âœ… **Zysk netto > 0** (u nas: -1,300 zÅ‚)
âœ… **Czas tuningu < wartoÅ›Ä‡ poprawy** (u nas: nie dotyczy)

### 4. **OSTATECZNA REKOMENDACJA**

```
ğŸ›‘ ZOSTAÅƒ PRZY MODELU PODSTAWOWYM

Powody:
- Model bazowy (bez tuningu) jest lepszy
- Tuning nie tylko nie pomÃ³gÅ‚, ale pogorszyÅ‚ wyniki
- Model bazowy: Recall 79.93%, Precision 79.03%
- Model tuned: Recall 79.41%, Precision 78.21%
```

## ğŸ’¡ Kluczowe lekcje

### Dla Data Scientists:

1. **Tuning â‰  Automatyczna poprawa** - czasem moÅ¼e pogorszyÄ‡ model
2. **Zawsze porÃ³wnuj PRZED vs PO** - nigdy nie zakÅ‚adaj, Å¼e tuning pomoÅ¼e
3. **Cross-validation jest kluczowe** - chroni przed overfittingiem
4. **Dobry baseline to podstawa** - model z domyÅ›lnymi ustawieniami moÅ¼e byÄ‡ wystarczajÄ…cy

### Dla biznesu:

1. **Nie kaÅ¼da "optymalizacja" siÄ™ opÅ‚aca** - czas ma wartoÅ›Ä‡
2. **Model prostszy moÅ¼e byÄ‡ lepszy** - mniej ryzyka, Å‚atwiejsze utrzymanie
3. **Metryki techniczne â‰  WartoÅ›Ä‡ biznesowa** - zawsze obliczaj ROI
4. **80% jakoÅ›ci w 20% czasu** - czÄ™sto wystarcza model bazowy

## ğŸ“ˆ PorÃ³wnanie z innymi projektami

W naszym portfolio mamy 3 modele churn:

| Projekt | Model | AUC | Recall | Czy tuning? | Wynik |
|---------|-------|-----|--------|-------------|-------|
| 04_churn_overfitting | Gradient Boosting | 0.8463 | 79.93% | âŒ Nie | âœ… Åšwietny! |
| **05_churn_tuning** | **GB (tuned)** | **0.8493** | **79.41%** | **âœ… Tak** | **âš ï¸ Gorszy!** |

**Wniosek:** Model z projektu 04 (bez tuningu) jest **LEPSZY** niÅ¼ model z tuningu!

## ğŸš€ Jak uruchomiÄ‡

### Instalacja

```bash
# Sklonuj repozytorium
git clone https://github.com/your-username/ML-portfolio.git
cd ML-portfolio/05_churn_model_tuning

# UtwÃ³rz Å›rodowisko wirtualne
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Zainstaluj zaleÅ¼noÅ›ci
pip install pandas numpy pycaret scikit-learn
```

### Uruchomienie

**Notebook (zalecane):**
```bash
jupyter notebook churn_tuning.ipynb
```

**Skrypt Python:**
```bash
python churn_tuning.py
```

### Struktura projektu

```
05_churn_model_tuning/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ churn_tuned_model.pkl  (zapisany model po tuningu)
â”‚
â”œâ”€â”€ churn_tuning.ipynb  (notebook z peÅ‚nymi wyjaÅ›nieniami)
â”œâ”€â”€ churn_tuning.py     (skrypt Python)
â””â”€â”€ README.md           (ten plik)
```

## ğŸ“š Dodatkowe materiaÅ‚y

### Co to jest tuning?

Tuning (optymalizacja hiperparametrÃ³w) to proces szukania **najlepszych ustawieÅ„** algorytmu ML:

```python
# PrzykÅ‚adowe hiperparametry dla Gradient Boosting:
- learning_rate: jak szybko model uczy siÄ™
- n_estimators: ile drzew decyzyjnych
- max_depth: gÅ‚Ä™bokoÅ›Ä‡ drzew
- min_samples_split: minimalna liczba prÃ³bek do podziaÅ‚u
```

### Metody tuningu

1. **Grid Search** - testuje wszystkie kombinacje (wolne)
2. **Random Search** - losowe kombinacje (szybsze)
3. **Optuna** - inteligentne przeszukiwanie (uÅ¼yte w tym projekcie)

### Metryki wyjaÅ›nione

- **Accuracy** = (TP + TN) / All - ogÃ³lna dokÅ‚adnoÅ›Ä‡
- **AUC** = pole pod krzywÄ… ROC - zdolnoÅ›Ä‡ rozrÃ³Å¼niania
- **Recall** = TP / (TP + FN) - ile % odchodzÄ…cych wykrywamy
- **Precision** = TP / (TP + FP) - ile % alertÃ³w jest trafnych

## ğŸ“ Wnioski koÅ„cowe

### GÅ‚Ã³wna konkluzja:

> **Tuning nie zawsze poprawia model. W tym przypadku model bazowy (bez tuningu) okazaÅ‚ siÄ™ LEPSZY.**

### Praktyczne rekomendacje:

1. âœ… **Zawsze trenuj model bazowy** - czÄ™sto jest wystarczajÄ…cy
2. âœ… **PorÃ³wnuj wyniki obiektywnie** - nie zakÅ‚adaj, Å¼e tuning pomoÅ¼e
3. âœ… **Obliczaj ROI** - czas to pieniÄ…dz
4. âœ… **Prostota > ZÅ‚oÅ¼onoÅ›Ä‡** - prostszy model Å‚atwiejszy w utrzymaniu

### Co dalej?

JeÅ›li szukasz **DZIAÅAJÄ„CEGO** modelu churn, sprawdÅº:
- **Projekt 04: churn_overfitting** - model bazowy z AUC 0.8463 i Recall 79.93%

Ten projekt pokazaÅ‚, Å¼e **nie wszystkie techniki ML zawsze dziaÅ‚ajÄ…** - to cenna lekcja! ğŸ¯

---

**Autor:** Åukasz  
**Data:** StyczeÅ„ 2026
